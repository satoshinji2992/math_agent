import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.legacy.callbacks import CallbackManager
from llama_index.llms.openai_like import OpenAILike
import matplotlib.pyplot as plt
import numpy as np
import re
import os

# Create an instance of CallbackManager
callback_manager = CallbackManager()

file_path = ".env"

# è¯»å–æ–‡ä»¶å†…å®¹
with open(file_path, 'r') as file:
    config = file.readlines()

# å°†æ¯è¡Œå†…å®¹æ‹†è§£ä¸ºé”®å€¼å¯¹
config_dict = {}
for line in config:
    if '=' in line:
        key, value = line.strip().split('=', 1)
        config_dict[key.strip()] = value.strip().strip('"')

# æ‰“å°è¯»å–çš„é…ç½®ä¿¡æ¯
api_base_url = config_dict.get("api_base_url")
model = config_dict.get("model")
api_key = config_dict.get("api_key")



llm =OpenAILike(model=model, api_base=api_base_url, api_key=api_key, is_chat_model=True,callback_manager=callback_manager)

st.set_page_config(page_title="æ•°å­¦åˆ†æåŠ©æ‰‹", page_icon="ğŸ¦œğŸ”—")
st.title("æ•°å­¦åˆ†æåŠ©æ‰‹")

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def init_models():
    embed_model = HuggingFaceEmbedding(
        model_name="/root/model/sentence-transformer"
    )
    Settings.embed_model = embed_model

    #ç”¨åˆå§‹åŒ–llm
    Settings.llm = llm

    documents = SimpleDirectoryReader("/root/math_agent/data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    return query_engine

# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if 'query_engine' not in st.session_state:
    st.session_state['query_engine'] = init_models()

def greet2(question):
    response = st.session_state['query_engine'].query(question)
    return response

# å‡½æ•°å›¾åƒç»˜åˆ¶ç›¸å…³åŠŸèƒ½
def plot_function(function_str, x_range=(-10, 10), num_points=1000):
    """
    ç»˜åˆ¶æ•°å­¦å‡½æ•°å›¾åƒ
    
    å‚æ•°:
    function_str: å‡½æ•°è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "x**2" æˆ– "np.sin(x)"
    x_range: xè½´èŒƒå›´ï¼Œé»˜è®¤ä¸º(-10, 10)
    num_points: ç”¨äºç»˜å›¾çš„ç‚¹æ•°ï¼Œé»˜è®¤ä¸º1000
    
    è¿”å›:
    matplotlibå›¾å½¢å¯¹è±¡
    """
    # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢
    fig, ax = plt.subplots(figsize=(8, 5))
    
    # åˆ›å»ºxè½´æ•°æ®ç‚¹
    x = np.linspace(x_range[0], x_range[1], num_points)
    
    try:
        # æ›¿æ¢å¸¸è§çš„æ•°å­¦å‡½æ•°è¡¨ç¤ºä¸ºnumpyå‡½æ•°
        func_str = function_str.replace('^', '**')
        for math_func in ['sin', 'cos', 'tan', 'exp', 'log', 'sqrt']:
            if math_func in func_str:
                func_str = func_str.replace(math_func, f'np.{math_func}')
        
        # è®¡ç®—yå€¼
        y = eval(func_str)
        
        # ç»˜åˆ¶å‡½æ•°
        ax.plot(x, y)
        ax.grid(True)
        ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(f'å‡½æ•°: {function_str}')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        
        return fig
    except Exception as e:
        st.error(f"ç»˜åˆ¶å‡½æ•°æ—¶å‡ºé”™: {str(e)}")
        return None

def is_plot_request(message):
    """
    æ£€æŸ¥ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦ä¸ºç»˜åˆ¶å‡½æ•°çš„è¯·æ±‚
    
    å‚æ•°:
    message: ç”¨æˆ·æ¶ˆæ¯å­—ç¬¦ä¸²
    
    è¿”å›:
    å¦‚æœæ˜¯ç»˜å›¾è¯·æ±‚ï¼Œè¿”å›å‡½æ•°è¡¨è¾¾å¼ï¼›å¦åˆ™è¿”å›None
    """
    plot_patterns = [
        r'ç”»å‡ºå‡½æ•°\s*[ï¼š:]\s*(.*?)$',
        r'ç»˜åˆ¶å‡½æ•°\s*[ï¼š:]\s*(.*?)$',
        r'ç”»å‡º\s*(.*?)çš„å›¾åƒ',
        r'ç»˜åˆ¶\s*(.*?)çš„å›¾åƒ',
        r'plot.*?function\s*[ï¼š:]\s*(.*?)$'
    ]
    
    for pattern in plot_patterns:
        match = re.search(pattern, message, re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return None
      
# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ•°å­¦åˆ†æåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿä¹Ÿå¯ä»¥è®©æˆ‘ç»˜åˆ¶å‡½æ•°å›¾åƒï¼\n ç»˜åˆ¶å›¾åƒè¯·ç”¨ä»¥ä¸‹æ ¼å¼:\n- 'ç”»å‡ºå‡½æ•°ï¼šx^2 + 2*x + 1'\n- 'ç»˜åˆ¶å‡½æ•°ï¼šsin(x)'\n- 'ç”»å‡ºcos(x)çš„å›¾åƒ'\n- 'ç»˜åˆ¶x^3 - 4*xçš„å›¾åƒ' "}]    

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ•°å­¦åˆ†æåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿä¹Ÿå¯ä»¥è®©æˆ‘ç»˜åˆ¶å‡½æ•°å›¾åƒï¼\n ç»˜åˆ¶å›¾åƒè¯·ç”¨ä»¥ä¸‹æ ¼å¼:\n- 'ç”»å‡ºå‡½æ•°ï¼šx^2 + 2*x + 1'\n- 'ç»˜åˆ¶å‡½æ•°ï¼šsin(x)'\n- 'ç”»å‡ºcos(x)çš„å›¾åƒ'\n- 'ç»˜åˆ¶x^3 - 4*xçš„å›¾åƒ' "}]

st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Function for generating LLaMA2 response
def generate_llama_index_response(prompt_input):
    return greet2(prompt_input)

# User-provided prompt
if prompt := st.chat_input("è¾“å…¥é—®é¢˜æˆ–è€…è®©æˆ‘ç»˜åˆ¶å‡½æ•°å›¾åƒ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç»˜å›¾è¯·æ±‚
    function_expr = is_plot_request(prompt)
    if function_expr:
        with st.chat_message("assistant"):
            st.write(f"æ­£åœ¨ç»˜åˆ¶å‡½æ•°: {function_expr}")
            fig = plot_function(function_expr)
            if fig:
                st.pyplot(fig)
                message = {"role": "assistant", "content": f"æˆ‘å·²ç»ç»˜åˆ¶äº†å‡½æ•° {function_expr} çš„å›¾åƒã€‚"}
                st.session_state.messages.append(message)
            else:
                error_msg = "æ— æ³•ç»˜åˆ¶è¯¥å‡½æ•°ï¼Œè¯·æ£€æŸ¥å‡½æ•°è¡¨è¾¾å¼æ˜¯å¦æ­£ç¡®ã€‚"
                st.write(error_msg)
                message = {"role": "assistant", "content": error_msg}
                st.session_state.messages.append(message)
    # å¦‚æœä¸æ˜¯ç»˜å›¾è¯·æ±‚ï¼Œä½¿ç”¨åŸæœ‰çš„å“åº”ç”Ÿæˆé€»è¾‘
    elif st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = generate_llama_index_response(prompt)
                placeholder = st.empty()
                placeholder.markdown(response)
        message = {"role": "assistant", "content": response}
        st.session_state.messages.append(message)